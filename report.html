<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<title>Kinematics | CS 184 Final Project Report</title>
<link rel="stylesheet" href="style.css">
<link rel="icon" href="penrosetriangle.svg">
</head>
<body class="sans">
<a href="./index.html">HOME</a> / <a href="./proposal.html">PROPOSAL</a> / <a href="./milestone.html">MILESTONE</a> / <b><a href="#">REPORT</a></b> / <a href="./demo.html">DEMO</a>
<br />
<br />
<br />
<br />
<h1 class="page-title">Kinematics</h1><h1>CS 184 Final Project Report</h1>
Eunice Chan | <i>August 14, 2020</i>
<br />
<br />
<h2>Abstract</h2>
<p>In this project, I explored the concept of kinematics, the field concerned with the motion of objects without reference to the forces causing them. To this end, I implemented forward kinematics, and inverse kinematics. Inverse kinematics is not as straightforward as forward kinematics and there have been many approaches to the task. I implemented a numerical-based approach, using the Levenberg-Marquardt algorithm, and a sampling-based approach, using the Sequential Monte Carlo method.</p>
<h2>Technical Approach</h2>
<h3>Environment</h3>
<h3>Scene</h3>
<p>To prepare for implementing the kinematics approaches, I had to have something to act upon, and some way to see it. After looking around, I settled on using <a href="https://threejs.org/">Three.js</a>. To familarize myself, I went through <a href="https://www.linkedin.com/learning/learning-3d-graphics-on-the-web-with-three-js">this course</a>. My notes are <a href="./three.html">here</a>.</p>
<p>It was a little rough getting started and getting to where I wanted to be. After starting and scrapping my code four times, I found an example in the <a href="https://threejs.org/docs/scenes/bones-browser.html">documentation</a> that was very similar to what I wanted to do, so I used it as starter code. Below is the example without any modifications.</p>
<iframe style="width:100%;height:50vw" src="https://threejs.org/docs/scenes/bones-browser.html"></iframe>
<h3>Model</h3>
<p>The second step was to import my model. I found this part to be unexpectedly challenginga and tried 4/5 different approaches and creating 3 models.</p>
<p>In my first approach, referencing <a href="https://www.youtube.com/watch?v=z0MJ2KdGPFE">this Youtube video</a> and <a href="https://www.youtube.com/watch?v=eEqB-eKcv7k">this Youtube video</a>, I modeled the figure in Blender, and rigged it. After creating the model, I had to load it into the scene. My initial plan was to create the model in Blender and export it as a Three.js native model with the Three.js model exporter plugin, as the videos had suggested. However, I found out that it was deprecated and that Three.js suggests loading in a <code>.GLTF</code> (Graphics Library Transmission Format) file instead.</p>
<p>A <code>.GLTF</code> file is a kind of file format for 3D scenes and models using the <code>JSON</code> (JavaScript Object Notation) format. It is intended for web use and therefore has a minimum file size and runtime processing by apps. I couldn't see any downsides to following Three.js' suggestion, so I created my model in Blender and exported it as a <code>.GLTF</code> file.
So, I decided to take their suggestion and export as a <code>.GLTF</code> file.</p>
<p>Unfortunately, I ran into unexpected difficulty loading it in because the Three.js Github repo has everything in modules, which I cannot use if I do not have a server. I wanted to avoid a server if I could, so that I could host on Github Pages. As a result, I spent a long time trying to find the piece of the code that could load in my file. I did not have much luck.</p>
<p>Fortunately, I had taken an online course I took to get up to speed on Three.js. In that course, there were exercise files to accompany the videos. Most importantly, the libraries the exercises depended on were included, and one of the files in the library held the code for loading in <code>.OBJ</code> files.</p>
<p>Therefore, my next approach was to convert the <code>.GLTF</code> file I had into a <code>.OBJ</code> file. With this approach, I was able to successfully load it in. However, while working with the model, I ran into issues accessing the model's armature. After searching online, I was unable to find much information. The model was relatively simple, so I decided it would be a better use of my time to create the model in Three.js instead of loading it. Thus, I dropped this approach.</p>
<p>Hence, I started my second approach. I decided to shape the model by scaling the bones so that a wavy shape would form. It went well. However, when I was implementing forward kinematics, I realized I could not pretend two bones were one. I had split up each segment of the arm into two so that I could put a swell in the middle of each segment, and had assumed the fact there were two bones wouldn't change anything, as long as I only translate the "real" bones. That was a misconception. As a result, I dropped this approach and moved on once again.</p>
<p>In my final approach, I used a <code>BoxBufferGeometry</code> and modified the vertices. This required a lot of trial and error as I had to figure out what was the order in which the vertices were given to me (it was left, right, top, bottom, front back), then caulating the vertices' positions based on the  layer they are at and whether they are part of the top/bottom or not. I calculated the number of points per layer and points per top/bottom given the  input settings which allowed me to shape the model in the desired general shape even with an arbitrary number of bones/joints, although I did not end up implementing such a feature.</p>
<p>At this point, it doesn't really look like an arm, but it looks much more like an arm than a simple rectangle, and the shape helps me identify the joints, so I chalked this up to a win and moved on.</p>
<table>
  <tr>
    <td>
      <img src="./images/blender_model.png" />
      <figcaption>Blender model</figcaption>
    </td>
    <td>
      <img src="./images/bone_model.png" />
      <figcaption>Bone scaling</figcaption>
    </td>
    <td>
      <img src="./images/geometry_model.png" />
      <figcaption>Manual positioning</figcaption>
    </td>
  </tr>
</table>
<h3>Debug/Interactive Features</h3>
<p>I also implemented a movable target point in the GUI for inverse kinematics. Essentially, in inverse kinematics, the problem statement is: given n end-effectors, and n target points, find a combination of parameters to apply to the bones to get the end-effectors to the target point or, if that is impossible, as close as possible.</p>
<p>In this project, I implemented a single end effector and thus needed a single target point to identify the goal position (in some implementations, a goal rotation is included, but I decided to stick with just position).</p>
<p>My first thought was to implement it with the arrow keys. However, I quickly realized that the arrow keys can only control two dimensions and there are three dimensions (four if you include time) in the demo. The arrows keys are also being use by the <code>OrbitControls</code> and I really enjoyed having the ability to move around the scene, so I was reluctant to part with it.</p>
<p>In the end, I decided to implement the controls to be WASD & <code>shift</code> + mouseMoveDirection (calculated by storing the previous mouse location and the current) with <code>space</code> being the key that resets its position.</p>
<p>I find it a little clunky, but it's much better than tediously dragging sliders around, so I decided to stick with this approach unless I think of something better. After several days of debugging and finding out my target-moving priorities, I removed this feaature and implemented the sliders that I had initially thought was clunky. In addition to the slides, I implemented a follow-mouse feature so that the target would follow the mouse around.</p>
<br />
<video style="width:100%;padding:0 10%;" muted playsinline autoplay loop>
    <source src="./images/target_demo2.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo2.mov">link to the video</a> instead.</p>
</video>
<br />
<p>Other things I did to set up the environment was expose some parameters for the user to play with. A full list of all the parameters are in the instructions on the <a href="./demo.html">demo page</a>.</p>
<h3>Kinematics</h3>
<h3>Forward Kinematics</h3>
<p>Forward kinematics was relatively straightforward once all the groundwork was done due to the data structure representing the mesh. Given the user input for the XYZ position/rotation of the joints that make up the armature of the model, I set the position and rotation properties of the joint, and Three.js abstracts away the calculations when rendering it.</p>
<p>Initially, I was actually calcuating the effects of the joint positions and rotations on a single mesh point's location for a part of the inverse kinematics (to convert the parameters to a location to be compared to the target position in the objective function). However, when I expanded the number and types of joints to both rotation and translation, I ran into issues with my calculations. After some debugging, I got it to the point where it works if all the joints were rotational, or all the joints were translational, but when they were mixed, the calculations were frequently wrong. After sinking far too much time on it, I decided to switch approaches leaning on the Three.js' library.</p>
<video style="width:100%;padding:0 10%;" muted playsinline autoplay loop>
    <source src="./images/FK_demo.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/FK_demo.mov">link to the video</a> instead.</p>
</video>
<h3>Inverse Kinematics: Levenberg-Marquardt Algorithm (Damped Least Squares)</h3>
<p>This optimization algorithm is the Gauss–Newton (GN) algorithm with the addition of a trust region to solve non-linear least squares problems. This is an iterative algorithm, so the choice of starting point may cause it to be trapped in a local minima or the global minima, which may happen in this use-case since inverse kinematics is riddled with local minima.</p>
<p>LM takes in as an input a cost function \(f\) and a parameter vector \(\beta\) initialized with some values and returns a local minimum of the cost function. If there is only one minimum, \(\beta\) can be initialized to anything. In contrast, if there are many local minima, it should be initialized to a vector close to the global minimum to avoid converging to local minima.</p>
<br />
<img src="http://www.brnt.eu/phd/img251.png" />
<br />
<br />
<p>We iterate until either the stopping criterion \(\text{STOP-CRIT}\) is reached, or the number of interations \(k\) is equal to the maximal number of iterations \(k < k_{max}\). The stopping criterion can be anything. Typically in LM, we stop when the change (in the solution, in the cost function, relative change, etc.) is very small, because that means it is sufficiently near the minimum. In each iterations, we update our estimated \(\beta\) to be \(\beta + \delta\).</p>
<p>In GN, \(\delta=(J^TJ)^{-1}Jf\) where J is the Jacobian of the function evaluated at x. LM modifies this by adding a damping factor. The resulting equations are called the augmented normal equations. \(\delta\) is thus defined as the solution to that linear system of equations: \((J^TJ + \lambda\text{diag}(J^TJ))\delta = Jf\) where \(\lambda\) is a positive damping parameter, which should be large so that it is well-behaved near singularities (although if it is too large, it will converge slowly). It acts as the Lagrange multipler for the constraint that each search is limited to the trust region radius.</p>
<p>Initially, Levenberg modified the system of equations to \((J^TJ + \lambda\text{diag}(I))\delta = Jf\) which had the issue of inverting \((J^TJ + \lambda\text{diag}(I))\) not being used at all when \(\lambda\) is large. In a 1971 paper titled <i>A modified Marquardt subroutine for non-linear least squares</i>, Fletcher modified this algorithm, creating the LM algorithm, in which each component of the gradient is scaled according to the curvature, so that there is larger movement along the directions where the gradient is smaller. In my implementation, I include the modification made by Nash in which \(\lambda\text{diag}(J^TJ)\) is replaced with \(\lambda\text{diag}(I + J^TJ)\) so that it is positive definite and symmetric so I can use Cholesky decomposition to solve the system.</p>
<p>The augmented matrix \(J^TJ + \lambda\text{diag}(I + J^TJ)\) has the property of positive definiteness making \(\delta\) a step in the descent direction. Thus, this algorithm is like gradient descent when \(\lambda\) is large, and like Gauss-Newton when \(\lambda\) is small, which is good because the convergence of the Gauss-Newton method can be almost quadratic near the minimum. The LM method tends to act similarly to the pseudoinverse method away from singularities and effectively smooths out the performance of pseudoinverse method in the neighborhood of singularities.</p>
<p>At each iteration, \(\beta\) is updated if the new version has a lower cost, and \(\lambda\) is updated by either increasing or decreasing by a constant \(v\) (typically set to 2), if there is an improvement or deprovement of the cost function respectively.</p>
<p>To simplify things, I will not consider joint constraints or self-collisions.</p>
<h3>Implementation</h3>
<p>I had an incredibly difficult time implementing it as the wording of the documents required very careful reading to get all the details. Additionally, the wording was vague around some terms, so I had to cross reference the papers to make educated guesses about what the authors meant.</p>
<p>I kept debugging and paring down the complexity of my implementation until I got a working implementation. This occurred when I used three translational joints with 1 degree of freedom each. The epiphany I had was while reading yet anotehr survey of IK methods. I read the Jacobian can be interpreted as an m by n matrix of positions or a 3k by n matrix of points where m = 3k. That finally answered my question about how to store a position as a single value in a matrix. Initially I was incredibly confused about how to matrix multiplication would work out, and I had assumed I had to put the positions in the matrix as a scalar, so I used Euclidean distance for residuals and for the Jacobian, took only the x, y, or z value, depending on the associated axis (essentially, treating the gradiant's cross product as a dot product because I thought they were using the cross loosely to mean any kind of multiplication) and put it in as the position to the matrix.</p>
<p>In the specific case of the arm model, there is one end effector as identified with the green sphere that is associated with the target point identified by the pink sphere.</p>
<p>The objective function is sum of squares. In this case, it is just one square because there is only one end effector. I am trying to minimize the squared distance between the target and the end effector.</p>
<p>\(J\) is the Jacobian matrix with each row being the position of the end effector and each element in the row being the partial deriviative of the end effector with respect to an element of the parameter. In the arm model case, since there is only one end effector, the Jacobian matrix is just a row vector.</p>
<p>Since I was using Three.js's <code>Vector3</code> and <code>Matrix3</code> instead of a dedicated linear algebra library, the functionality was very limited and I had to do a lot of manual calculations as well as was limited to vectors of length three and matrices of length three by three. This was part of the reason why I decided to limit my parameters to three. However, now that I've finished this implementation, I do not think these restrictions is worth the easy integration with the rest of the Three.js parts of my code.</p>
<figure>
<video style="width:100%;padding:0 10%;" muted playsinline autoplay loop>
    <source src="./images/IK_translation_LM.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
    <figcaption>Using LM to perform IK</figcaption>
</figure>
<p>Some notes about the demo:</p>
<ul>
  <li>The black line is there to visualize the distance between the target point and the endpoint (associated end effector).</li>
  <li>The <code>One Step</code> button specifically divides \(\lambda\) once and multiplies \(\lambda\) <code>maxIter</code> times or until an improvement to the objective function is made, whichever comes first.</li>
  <li><code>Repeat</code> runs <code>One Step</code> once every render cycle. Seeing the mesh exactly follow the target is a little underwhelming, so I linearly interpolate the current mesh joint parameters and the joint parameters calculated by LM so it would smoothly follow the target.</li>
</ul>
<p>In the first example in the demo, LM takes 3 steps. You can see that each time I run one step, it modifies the bone parameters (joint 1x-translation, joint 2 y-translation, joint 3 z-translation) to the end effector closer to the target point until it reached that target point/minimum.</p>
<p>In the second example, it reaches the target point immediately in one step.</p>
<p>The example after that, I sent multiple target points and it is able to follow along smoothly, meaning that the IK calculations are able to be done in a real-time environment (if speed is set to 1, it will always overlap the target point. However, for illustrative purposes, I set the speed to 0.1, which allows the target and end effector point to be clearly seen.)<p>
<p>The parameters such as <code>lambda</code> and <code>maxIter</code> is in the GUI for anyone to manipulate. However, as it converges very quickly, it makes no notable difference.</p>
<h3>Inverse Kinematics: Sequential Monte Carlo Method (Particle Filter)</h3>
<p>Particle filters, also known as sequential Monte Carlo methods are simulation-based methods for calculating approximations to posterior distributions. The are preferred to related methods, such as Kalman filter because they avoid making linearity or normality assumptions which may not be true.</p>
<p>The algorithm has roughly three major steps:</p>
<ol>
  <li>Initialization. Draw N particles from a probability distribution.</li>
  <li>Importance sampling. Add \(\tilde x_t^{(i)}\) drawn from a probability distribution \(p(x_t^{(i)}|x_{t-1}^{(i)})\) to a set \(\tilde x_{0:t}^{(i)}\). Then, calculate the importance weights and normalize them.</li>
  <li>Resampling. Draw N samples with replacement from the weighted set \(\tilde x_{0:t}^{(i)}\).</li>
</ol>
<h3>Implementation</h3>
<p>To use this algorithm for IK, each render runs one iteration of this and take the parameter that has the heaviest weight associated with it.</p>
<p>Initially, I had planned to faithfully follow <a href="http://perception.inrialpes.fr/Publications/2008/CA08/amdo08.pdf">this paper</a>. However, as I had to submit soon and there were further major changes I wanted to add to the demo, and my attempt to faithfully follow the paper using the LM algorithm was a bit of a headache, I decided a good compromise would be to simplify the algorithm.</p>
<p>Following that train of thought, I took inspiration from that paper and the general particle filter algorithm (which I got a high-level understanding of <a href="https://jblevins.org/notes/smc-intro">here</a>).</p>
<p>The steps are as follows:</p>
<ol>
  <li>Explore the state space with N copies of the system (particles). For \(i = 1...N\), draw \(Q^{(i)}_k \sim p(Q_k|Q^{(i)}_{k−1})\) where \(p(Q_k|Q^{(i)}_{k−1})\) is the evolution prior. We do this by rejection sampling from a distribution (centered at Q^{(i)}_{k−1}) if there's no a prior knowledge. Otherwise, change the mean to reflect the knowledge) until the sample is accepted. Put hard constraints like joint limits or self-intersection avoidance here. The main advantage of using SMCM is that it deals with inequality constraints simply--either accepting or rejecting.</li>
  <ul>
    <li>In the demo, I sample uniformly at random N times near the current position from the sample space and since there is no constraint, I do not reject any sample.</li>
  </ul>
  <li>Calculate the weights to be equal to the \(w^{(i)}_k \propto p(y_k|Q^{(i)}_k)\), then normalize them to sum to 1. Here, \(p(y_k|Q^{(i)}_k)\) is the likelihood. It acts like the objective function and gives an evaluation of how good the configuration is with respect to the desired task. For example, one can set likelihood to be the distance between the end effector configuration and the desired configuration. Put soft or numerical constraints (such as minimizing a given function) here.</li>
  <li>Set the mesh position to be the MAP estimate (the particle with the greatest probability). To smooth out the motion, interpolate.</li>
  <li>Mutate and reselect the particles.</li>
</ol>
<p>Since these are random samples, it's very Here, the speed slider's linear interpolations serves to smooth the trajectory.</p>
<br />
<table style="width:80vw;margin:0 -10vw">
  <tr>
    <td style="padding:1% 1%">
      <video style="width:100%" muted playsinline autoplay loop>
          <source src="./images/SMCM_nosmooth_100.mov" />
          <p>Your browser doesn't support my video. Here is a <a href="./images/SMCM_nosmooth_5.mov">link to the video</a> instead.</p>
      </video>
      <figcaption>Particle count n = 100, no smoothing</figcaption>
    </td>
    <td style="padding:1% 1%">
      <video style="width:100%" muted playsinline autoplay loop>
          <source src="./images/SMCM_nosmooth_10000.mov" />
          <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
      </video>
      <figcaption>Particle count n = 10000, no smoothing</figcaption>
    </td>
  </tr>

  <tr>
    <td style="padding:1% 1%">
      <video style="width:100%" muted playsinline autoplay loop>
          <source src="./images/SMCM_smooth_100.mov" />
          <p>Your browser doesn't support my video. Here is a <a href="./images/SMCM_smooth_5.mov">link to the video</a> instead.</p>
      </video>
      <figcaption>Particle count n = 100, smoothing</figcaption>
    </td>
    <td style="padding:1% 1%">
      <video style="width:100%" muted playsinline autoplay loop>
          <source src="./images/SMCM_smooth_10000.mov" />
          <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
      </video>
      <figcaption>Particle count n = 10000, smoothing</figcaption>
    </td>
  </tr>
</table>

<h2>Results</h2>
<p><a href="./demo.html">Demo page here.</a></p>
<iframe style="width:100%;height:50vw" src="./demo/ikajdc.html"></iframe>
<p>This demo took about half a week to make after I finished implementing the version as seen in the SMCM section. That is because I decided to completely refactor the code. Before, I was using Three.js' Vector3 class for the parameters which limited the number of parameters I could include to three. Since I was refactoring it anyway, I decided to allow the user to choose which constraints to use. I rewrote the numerical calculations for LM completely, and reinitialized the particles each time in the SMCM because of the issue that once the user changes which parameters are enabled, the old particles values are invalid because they are based on the previous definition of the parameters.</p>
<h2>Conclusion</h2>
<p>Working on this project reinforced a lot of concepts I learned in the class. One specific incident that stood out to me was when I was creating the model. At one point, I had considered defining the model from scratch. I read into how to do that and saw that it used the halfedge data structure and was extremely similar to what I did in project 2, MeshEdit. In the end, I didn't choose to create the model completely from scratch. However, I had the confidence that I was well equipped to do such a task, if need be. The concepts I was exposed to in class such as FOV, world-to-object/object-to-world transform, rendering, and so on showed up time and time again while working on this project and the background I built during the class allowed me to confidently navigate through and debug my code.</p>
<p>I also learned a lot of general project skills such as balancing my desire for perfection and my desire for getting things done on time. There are a lot of paths that I could take going forward on this project: adding more features, going back and refactoring and optimizing the implemented parts again, adding the other IK methods I was interested in, applying the concepts in another isolated component, and so on. At every step I was torn between moving on or continuing to work on the feature to make it better and at every step I had to weigh the additional benefits of spending more time on something, be the benefit learning, a better user experience, or making things easier down thee road, with the benefits of moving on and the reality of a deadline. I was constantly either extremely optimistic or extremely pessimistic with my ability to complete my set plans. When I finish a feature, figure out a bug, or when I initially start, I'm on top of the world, and was certain I would be able to achieve all my goals. When I'm hours into debugging an implementation or combing papers for answers to points of confusions I had regarding certain aspetcs of an algorithm, I was laughing at my foolish optimism and uncertain if I could even finish finding the bug by the deadline.</p>
<p>An example of the two states can be found in my initial project proposal and in my milestone writeup. In the project proposal, I was excited about inverse kinematics, knew nothing about inverse kinematics, and did not understand the concept of time. In the milestone, I realized I was halfway through the project, time-wise, had simplified my initial vision for the constraints to be just translation to a single axes for each joint, and was unsure I would even be able to get a working IK example out. This experience taught me a lot about iterations and scaling things down to the basics to get the key concepts. I was able to complete the rest of the project with the simplified model, then reimplement everything with more complexity.</p>
<p>Although initially, I had planned to implement IK with stylization in the proposal, my interests shift and I was focused more on improving and expanding my implementations of the IK methods. It was a fantastic albiet not very healthy or sustainable two weeks of working until 4 am, falling asleep, waking up, going to lecture, working, and repeating because I was so excited to work on the project. I learned so much in the past two weeks, building upon the foundations in computer graphics and imaging I got in the course. I learned a whole new framework (Three.js), and implemented not one, but two IK methods over the course of two weeks. I was really excited to learn about SMCM because we had learned about Monte Carlo in the context of sampling light. I remember reading the term importance sampling and connecting that with what I did in a previous project, which was immensely helpful in comprehending the concepts with which I was presented. I learned a lot of skills and knowedge during this project and in the course as a whole, both computer graphics-wise and otherewise. I look forward to using these skills in the future when I do more graphics-intensive projects, or when I am reading technical papers, or even when I'm planning and setting goals. I can find many aspects of my life to which I can apply what I have learned in this course.</p>

<h2>Contributions</h2>
<p>Although the final project is typically done in groups of three, this offering of the course is in no way typical. Due to the fast pace of a summer version of the course and the fact it is fully online due to COVID, the students of this course were allowed to work on the final project alone. As I wanted to gain a thorough experience and understanding of kinematics, I decided to undertake this project alone.</p>

<h2>References</h2>
<p>Of course, I was not truly alone, as I had the entire Internet at my disposal. Here is a list in no particular order of the material I found the most helpful.</p>
<ul>
  <li>
    <a href="https://threejs.org/">https://threejs.org/</a>
  </li>
  <li>
    <a href="https://www.linkedin.com/learning/learning-3d-graphics-on-the-web-with-three-js">https://www.linkedin.com/learning/learning-3d-graphics-on-the-web-with-three-js</a>
  </li>
  <li>
    <a href="https://medium.com/unity3danimation/overview-of-jacobian-ik-a33939639ab2">https://medium.com/unity3danimation/overview-of-jacobian-ik-a33939639ab2</a>
  </li>
  <li>
    <a href="http://andreasaristidou.com/publications/papers/IK_survey.pdf">http://andreasaristidou.com/publications/papers/IK_survey.pdf</a>
  </li>
  <li>
    <a href="http://math.ucsd.edu/~sbuss/ResearchWeb/ikmethods/iksurvey.pdf">http://math.ucsd.edu/~sbuss/ResearchWeb/ikmethods/iksurvey.pdf</a>
  </li>
  <li>
    <a href="http://perception.inrialpes.fr/Publications/2008/CA08/amdo08.pdf">http://perception.inrialpes.fr/Publications/2008/CA08/amdo08.pdf</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=z0MJ2KdGPFE">https://www.youtube.com/watch?v=z0MJ2KdGPFE</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=eEqB-eKcv7k">https://www.youtube.com/watch?v=eEqB-eKcv7k</a>
  </li>
  <li>
    <a href="http://www.brnt.eu/publications/brunet2010phd.pdf">http://www.brnt.eu/publications/brunet2010phd.pdf</a>
  </li>
  <li>
      <a href="https://www.researchgate.net/publication/273166356_Inverse_Kinematics_a_review_of_existing_techniques_and_introduction_of_a_new_fast_iterative_solver#pf1d">Aristidou, Andreas & Lasenby, Joan. (2009). Inverse Kinematics: a review of existing techniques and introduction of a new fast iterative solver. </a>
  </li>
  <li>
      <a href="https://www.researchgate.net/publication/224240315_Solvability-Unconcerned_Inverse_Kinematics_by_the_Levenberg-Marquardt_Method">Sugihara, Tomomichi. (2011). Solvability-Unconcerned Inverse Kinematics by the Levenberg–Marquardt Method. Robotics, IEEE Transactions on. 27. 984 - 991. 10.1109/TRO.2011.2148230.</a>
  </li>
</ul>

<h2>Statistics</h2>
<p><a href="https://github.com/eunice-chan/CS184-Final-Project">Github Repo</a></p>
<ul>
  <li>Created August 1, 2020.</li>
  <li>Committed daily from August 1 til August 12.</li>
  <li>5 branches.</li>
  <li>74 total commits.</li>
  <li>3 distinct versions of the demo.</li>
</ul>

<h2>Presentation</h2>
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRdANVCPrAPs5GX2cMyrskZjyslADNU4-Up5Enqsm5b0rB4_izx_cd-HUCOeREwTeyT-mocFsrYM9Lg/embed?start=true&loop=true&delayms=10000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

</body>
</html>
