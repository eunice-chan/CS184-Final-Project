<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<title>Inverse Kinematics | CS 184 Final Project Notes</title>
<link rel="stylesheet" href="style.css">
<link rel="icon" href="penrosetriangle.svg">
</head>

<body class="sans">
<a href="./index.html">HOME</a> / <a href="./proposal.html">PROPOSAL</a> / <a href="./milestone.html">MILESTONE</a> / <b><a href="#">NOTES</a></b> / <a href="./report.html">REPORT</a> / <a href="./demo.html">DEMO</a>
<br />
<br />
<br />
<br />
<h1 class="page-title">Inverse Kinematics</h1><h1>CS 184 Final Project Notes</h1>
Eunice Chan | <i>August 7, 2020</i>
<br />
<br />
<h2>Part 1: Interactive Interface</h2>
<h3>Model</h3>
<h3>Different Model Versions</h3>
<table>
  <tr>
    <td>
      <img src="./images/blender_model.png" />
      <figcaption>Version 1: Blender model</figcaption>
    </td>
    <td>
      <img src="./images/bone_model.png" />
      <figcaption>Version 2: Bone scaling</figcaption>
    </td>
    <td>
      <img src="./images/geometry_model.png" />
      <figcaption>Version 3: Manual positioning</figcaption>
    </td>
  </tr>
</table>
<br />
<p>The first thing I did was create the model. I found this part to be unexpectedly challenging. My initial idea was to create the model in Blender and export it as a Three.js native model with the Three.js model exporter plugin. However, I found out that it was deprecated and that Three.js suggests loading in a <code>.GLTF</code> (Graphics Library Transmission Format) file instead.</p>
<p>A <code>.GLTF</code> file is a kind of file format for 3D scenes and models using the <code>JSON</code> (JavaScript Object Notation) format. It is intended for web use and therefore has a minimum file size and runtime processing by apps. I couldn't see any downsides to following Three.js' suggestion, so I created my model in Blender and exported it as a <code>.GLTF</code> file.
So, I decided to take their suggestion and export as a <code>.GLTF</code> file.</p>
<p>Unfortunately, I ran into unexpected difficulty loading it in because the Three.js Github repo has everything in modules, which I cannot use if I do not have a server. I wanted to avoid a server if I could, so that I could host on Github Pages. As a result, I spent a long time trying to find the piece of the code that could load in my file. I did not have much luck.</p>
<p>Fortunately, I had taken an online course I took to get up to speed on Three.js. In that course, there were exercise files to accompany the videos. Most importantly, the libraries the exercises depended on were included, and one of the files in the library held the code for loading in <code>.OBJ</code> files.</p>
<p>Therefore, my next approach was to convert the <code>.GLTF</code> file I had into a <code>.OBJ</code> file. With this approach, I was able to successfully load it in, but I had to start up a server to do so as I got an error:</p>
<br />
<div class="code code-wrap">Access to XMLHttpRequest at 'path/to/models/arm.obj' from origin 'null' has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: http, data, chrome, chrome-extension, https.
</div>
<br />
<p>I continued forward and ran into issues accessing the model's armature. I looked around online but did not find much information. I decided to drop this approach because</p>
<ol>
  <li>I had to use a server. From what I understood, it's only necessary if it was a local file, but I didn't want to delve too deep into this</li>
  <li>The model was relatively simple, so I suspected it would be faster to instead make the model in Three.js.</li>
</ol>
<p>Hence, I started my second approach. I decided to shape the model by scaling the bones so that a wavy shape would form. It went well. However, when I was implementing forward kinematics, I realized I could not pretend two bones were one. I had split up each segment of the arm into two so that I could put a swell in the middle of each segment, and had assumed the fact there were two bones wouldn't change anything, as long as I only translate the "real" bones. That was a misconception.</p>
<p>Thus, I moved on to my final approach. I iterated over every vertex and set it's position. Below is the commented function that I did positioned the points in:</p>
<div class="code code-wrap">function createGeometry( sizing ) {

  // Sets the ratio and size of the rectangle
  var width = 5;
  var depth = 5;

  // Sets the resolution
  var widthSegments = 5;
  var depthSegments = 5;


	var geometry = new THREE.BoxBufferGeometry(
		width,
		sizing.height, // height
		depth,
		widthSegments,
		sizing.segmentCount * 2, // heightSegments = segmentCount * numberOfDimensions
		depthSegments
	);

  // Update the positions
	var position = geometry.attributes.position;
  position.needsUpdate = true;

  var vertex = new THREE.Vector3();

  for ( var i = 0; i < position.count; i ++ ) {

    // Get the position of the i-th vertex
    vertex.fromBufferAttribute( position, i );

    // Layer: each horizontal loop/segment
    // The number of points in each layer's edge for the front, back, left, and right faces
    var pointsPerLayer = ( sizing.segmentCount * 2 ) + 1;
    pointsPerLayer = pointsPerLayer * ( widthSegments + 1);

    // The points in the top and bottom faces
    var pointsPerTop = widthSegments + 1;
    pointsPerTop = pointsPerTop * ( widthSegments + 1);

    // Total number of layers
    var maxLayer = ( sizing.segmentCount ) * 2 + 1;
    var layer;

    if (i < 2 * ( pointsPerLayer + pointsPerTop ) ) {

      layer = ( Math.floor( i / ( widthSegments + 1 ) ) ) % maxLayer;

    } else {


      // Remove everything before because top and bottom faces have a different number of points and throws off the layer order.
      var j = i - 2 * ( pointsPerLayer + pointsPerTop );
      layer = ( Math.floor( j / ( widthSegments + 1 ) ) ) % maxLayer;

    }

    // Left: ( i < pointsPerLayer )
    // Right: ( i < 2 * pointsPerLayer )

    // Top: ( i < 2 *  pointsPerLayer + pointsPerTop  )
    // Bottom: ( i < 2 * ( pointsPerLayer + pointsPerTop ) )

    // Front: ( i < 3 * pointsPerLayer + 2 * pointsPerTop )
    // Back: ( i < 4 * pointsPerLayer + 2 * pointsPerTop )
    switch ( true ) {

      // Left & Right || Front & Back
      case ( i < 2 * pointsPerLayer ) || ( i >= 2 * ( pointsPerLayer + pointsPerTop ) ):

        if ( layer % 2  == 0 ) {

          position.setXYZ( i, vertex.x / 6, vertex.y, vertex.z / 6 );

        } else {

          position.setXYZ( i, vertex.x * layer / 5, vertex.y, vertex.z * layer / 5 );

        }

        break;

      // Top & Bottom
      default:

        position.setXYZ( i, vertex.x / 6, vertex.y, vertex.z / 6 );

        break;

    }
	}

  // Set skin indices and weights
	...

	return geometry;

}
</div>
<p>Once I got the positioning done, that was all I had to do as I had already rigged the model and added it to the control panel when I was working with the bone scaling model version. I did not have to change it much.</p>
<h2>Demo</h2>
<p>I used <a href="https://threejs.org/docs/#api/en/objects/SkinnedMesh">https://threejs.org/docs/#api/en/objects/SkinnedMesh</a> as starter code. It provided to me a basic scene setup, which I then modified. I altered the number/positioning of lights, some of the control parameters, drastically modified geometry of the model, and altered the user-facing variable controller to be associated with the correct joint parameters of the model. Here it is below: </p>
<a href="https://threejs.org/docs/scenes/bones-browser.html">https://threejs.org/docs/scenes/bones-browser.html</a>
<iframe style="width:100%;height:50vw" src="https://threejs.org/docs/scenes/bones-browser.html"></iframe>
<h3>Forward Kinematics</h3>
<p>Although I had initially written forward kinematics as a reach goal, I realized it wasn't very difficult, and would be helpful for checking if I rigged the model correctly.</p>
<p>Implementing forward kinematics was straightforward. Forward kinematics is defined as the process of obtaining the velocity and position of the end effectors given the input of the joint angles and angular velocities. In this case, I take in the user input which is the rotation or position of the joint and apply it to the model, transforming it from one pose to another.</p>
<p>In the demo, I constrained the user input so it won't be too overwhelming. I anchored the model in one position and gave it the approximate constraints of an arm.</p>
<br />
<video style="width:100%;padding:0 10%;" preload controls>
    <source src="./images/FK_demo.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/FK_demo.mov">link to the video</a> instead.</p>
</video>
<h3>Moveable Target Point</h3>
<p>The second major part of the demo is that I need to be able to set the target point. My first thought was to implement it with the arrow keys. However, I quickly realized that the arrow keys can only control two dimensions and there are three dimensions (four if you include time) in the demo. The arrows keys are also being use by the <code>OrbitControls</code> and I really enjoyed having the ability to move around the scene, so I was reluctant to part with it.</p>
<p>In the end, I decided to implement the controls to be WASD & <code>shift</code> + mouseMoveDirection (calculated by storing the previous mouse location and the current) with <code>space</code> being the key that resets its position.</p>
<p>I find it a little clunky, but it's much better than tediously dragging sliders around, so I decided to stick with this approach unless I think of something better.</p>
<br />
<video style="width:100%;padding:0 10%;" preload controls>
    <source src="./images/target_demo.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
</video>
<br />
<p>At this point, it looks kind of like <a href="https://d.ibtimes.co.uk/en/full/1567896/pokemon-sun-and-moon-starter-evolutions.png?w=736">Popplio</a> so you're free to consider this a  seal instead of an arm.</p>
<h3>The Levenberg–Marquardt (LM) Damped Least-Squares (DLS) Algorithm</h3>
<p>This optimization algorithm is the Gauss–Newton (GN) algorithm with the addition of a trust region to solve non-linear least squares problems. This is an iterative algorithm, so the choice of starting point may cause it to be trapped in a local minima or the global minima, which may happen in this use-case since inverse kinematics is riddled with local minima.</p>
<p>LM takes in as an input a cost function \(f\) and a parameter vector \(\beta\) initialized with some values and returns a local minimum of the cost function. If there is only one minimum, \(\beta\) can be initialized to anything. In contrast, if there are many local minima, it should be initialized to a vector close to the global minimum to avoid converging to local minima.</p>
<br />
<img src="http://www.brnt.eu/phd/img251.png" />
<br />
<br />
<p>We iterate until either the stopping criterion \(\text{STOP-CRIT}\) is reached, or the number of interations \(k\) is equal to the maximal number of iterations \(k < k_{max}\). The stopping criterion can be anything. Typically in LM, we stop when the change (in the solution, in the cost function, relative change, etc.) is very small, because that means it is sufficiently near the minimum. In each iterations, we update our estimated \(\beta\) to be \(\beta + \delta\).</p>
<p>In GN, \(\delta=(J^TJ)^{-1}Jf\) where J is the Jacobian of the function evaluated at x. LM modifies this by adding a damping factor. The resulting equations are called the augmented normal equations. \(\delta\) is thus defined as the solution to that linear system of equations: \((J^TJ + \lambda\text{diag}(J^TJ))\delta = Jf\) where \(\lambda\) is a positive damping parameter, which should be large so that it is well-behaved near singularities (although if it is too large, it will converge slowly). It acts as the Lagrange multipler for the constraint that each search is limited to the trust region radius.</p>
<p>Initially, Levenberg modified the system of equations to \((J^TJ + \lambda\text{diag}(I))\delta = Jf\) which had the issue of inverting \((J^TJ + \lambda\text{diag}(I))\) not being used at all when \(\lambda\) is large. In a 1971 paper titled <i>A modified Marquardt subroutine for non-linear least squares</i>, Fletcher modified this algorithm, creating the LM algorithm, in which each component of the gradient is scaled according to the curvature, so that there is larger movement along the directions where the gradient is smaller. In my implementation, I include the modification made by Nash in which \(\lambda\text{diag}(J^TJ)\) is replaced with \(\lambda\text{diag}(I + J^TJ)\) so that it is positive definite and symmetric so I can use Cholesky decomposition to solve the system.</p>
<p>The augmented matrix \(J^TJ + \lambda\text{diag}(I + J^TJ)\) has the property of positive definiteness making \(\delta\) a step in the descent direction. Thus, this algorithm is like gradient descent when \(\lambda\) is large, and like Gauss-Newton when \(\lambda\) is small, which is good because the convergence of the Gauss-Newton method can be almost quadratic near the minimum. The LM method tends to act similarly to the pseudoinverse method away from singularities and effectively smooths out the performance of pseudoinverse method in the neighborhood of singularities.</p>
<p>At each iteration, \(\beta\) is updated if the new version has a lower cost, and \(\lambda\) is updated by either increasing or decreasing by a constant \(v\) (typically set to 2), if there is an improvement or deprovement of the cost function respectively.</p>
<p>To simplify things, I will not consider joint constraints or self-collisions.</p>
<h3>Implementing Levenberg–Marquardt</h3>
<h3>Setting Up</h3>
<p>Although the algorithm is relatively straightforward, the computer graphics portion was a bit more complicated, so I had to implement many helper functions so that I could get the values I needed easily.</p>
<p>One thing I realized was that I needed to get the position of the endpoint and that would require a lot of calculations. Initially I decided to subset the vertices of the geometry and take only the ones on the top plane, then finding the mean position. However, while testing my work, I encountered the issue of it only working if it did not change from the initial pose. After some debugging, I realized that the points did not change when I transformed the bones, so I switched up my tactics.</p>
<p> Next, I created a sphere and attached it to the last bone, so I can visualize the end point as well as conveniently get it by accessing the sphere's location. When testing this feature, I found out that the position never changes as well. I found out that this was because the points were in object space while I was interested in its position in world space. Because of that, I added a function to transform from object to world space. Finally, I was able to successfullly get the coordinates of the endpoint. However, after much testing, I realized I needed the default position as beta is relative to that value, so I saved the default position of the endpoint without any transformations applied, and accessed the world coordinates of that.</p>
<p>The next step was to calculate the endpoint's position given the initial position and the transformation on each joint of its parent. To do this, I go through each joint, and apply the transformation relative to the joint on thhe endpoint.</p>
<figure>
<video style="width:100%;padding:0 10%;" preload controls>
    <source src="./images/point_prediction_demo.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
    <figcaption>Demo of point prediction and walking through the code</figcaption>
</figure>
<h3>Implementation</h3>
<p>I had an incredibly difficult time implementing it as the wording of the documents required very careful reading to get all the details. Additionally, the wording was vague around some terms, so I had to cross reference the papers to make educated guesses about what the authors meant.</p>
<p>I kept debugging and paring down the complexity of my implementation until I got a working implementation. This occurred when I used three translational joints with 1 degree of freedom each. The epiphany I had was while reading yet anotehr survey of IK methods. I read the Jacobian can be interpreted as an m by n matrix of positions or a 3k by n matrix of points where m = 3k. That finally answered my question about how to store a position as a single value in a matrix. Initially I was incredibly confused about how to matrix multiplication would work out, and I ended up assuming I had to put the positions in the matrix as a scalar, so I used Euclidean distance for residuals and for the Jacobian, took only the x, y, or z value, depending on the associated axis (essentially, treating the gradiant's cross product as a dot product because I thought they were using the cross loosely to mean any kind of multiplication) and put it in as the position to the matrix.</p>
<!-- <p>In our case, we want to minimize the square error between our guess ( \(\beta\) ) and the goal (the target point).</p> -->
<p>In the specific case of the arm model, there is one end effector as identified with the green sphere that is associated with the target point identified by the pink sphere.</p>
<p>The objective function is sum of squares. In this case, it is just one square because there is only one end effector. I am trying to minimize the squared distance between the target and the end effector.</p>
<p>\(J\) is the Jacobian matrix with each row being the position of the end effector and each element in the row being the partial deriviative of the end effector with respect to an element of the parameter. In the arm model case, since there is only one end effector, the Jacobian matrix is just a row vector.</p>
<p>Since I was using Three.js's <code>Vector3</code> and <code>Matrix3</code> instead of a dedicated linear algebra library, the functionality was very limited and I had to do a lot of manual calculations as well as was limited to vectors of length three and matrices of length three by three. This was part of the reason why I decided to limit my parameters to three. However, now that I've finished this implementation, I do not think these restrictions is worth the easy integration with the rest of the Three.js parts of my code.</p>
<!-- Does not handle arbitrary intial guesses well -->
<figure>
<video style="width:100%;padding:0 10%;" preload controls>
    <source src="./images/IK_translation_LM.mov" />
    <p>Your browser doesn't support my video. Here is a <a href="./images/target_demo.mov">link to the video</a> instead.</p>
    <figcaption>Using LM to perform IK</figcaption>
</figure>
<p>Some notes about the demo:</p>
<ul>
  <li>The black line is there to visualize the distance between the target point and the endpoint (associated end effector).</li>
  <li>The <code>One Step</code> button specifically divides \(\lambda\) once and multiplies \(\lambda\) <code>maxIter</code> times or until an improvement to the objective function is made, whichever comes first.</li>
  <li><code>Repeat</code> runs <code>One Step</code> once every render cycle. Seeing the mesh exactly follow the target is a little underwhelming, so I linearly interpolate the current mesh joint parameters and the joint parameters calculated by LM so it would smoothly follow the target.</li>
</ul>
</body>
</html>
